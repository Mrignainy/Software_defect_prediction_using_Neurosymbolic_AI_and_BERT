# -*- coding: utf-8 -*-
"""my BERT code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ip7P04lAG1h9iVF4mDsfHtLvNzDAFi29
"""

import nltk
nltk.download('stopwords')

# Core dependencies
!pip install numpy==1.26.4 scipy==1.11.4

# NLP & visualization stack
!pip install nltk==3.8.1
!pip install spacy==3.7.4
!pip install gensim==4.3.2
!pip install pyldavis==3.4.1
!pip install wordcloud==1.9.3

# BERTopic + extras
!pip install bertopic==0.16.4
!pip install bertopic[visualization]
!pip install bertopic[flair]
!pip install bertopic[gensim]
!pip install bertopic[spacy]
!pip install bertopic[use]
!pip install bertopic[vision]

# Sentence embeddings
!pip install sentence-transformers==2.7.0

# NLTK data
!python -m nltk.downloader all

import re
import numpy as np
import pandas as pd
from pprint import pprint
from copy import deepcopy
from bertopic import BERTopic

import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
from gensim.models.ldamodel import LdaModel
from gensim import models

#from gensim.models.ldamulticore import LdaMulticore
from umap import UMAP
from sentence_transformers import SentenceTransformer

from bertopic import BERTopic

import spacy
from nltk.corpus import stopwords

import spacy
from nltk.stem.snowball import SnowballStemmer

import pyLDAvis
import pyLDAvis.gensim
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

# nltk Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

print (stop_words)

import pandas as pd
import spacy

# Load the dataset
df = pd.read_excel('Software defect Prediction.xlsx', usecols=['Publication Year', 'Title', 'Abstract Note'], sheet_name='Sheet1')

# Combine "Title" and "Abstract Note" into a single text per row
# Fill NaN values with an empty string to avoid errors during concatenation
df['combined_text'] = df['Title'].fillna('') + " " + df['Abstract Note'].fillna('')

# Convert the combined texts to a list
data = df['combined_text'].tolist()

print(data[0:5])

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
model = BERTopic(calculate_probabilities=True, embedding_model=embedding_model)

topics, probs = model.fit_transform(data)

model.get_topic_freq()

# Get topic frequency
topic_freq = model.get_topic_freq()

# Convert to DataFrame (if necessary)
topic_freq_df = pd.DataFrame(topic_freq)

# Save as Excel
topic_freq_df.to_excel("topic_freq.xlsx", index=False)

model.get_topic_info()

# Assuming `model.get_topic_info()` returns a DataFrame-like structure
topic_info = model.get_topic_info()

# Save to Excel
topic_info.to_excel("topic_info.xlsx", index=False)

import bertopic
print(bertopic.__version__)

# Generate labels
topic_labels = model.generate_topic_labels(nr_words=10, topic_prefix=False, word_length=15, separator= "_")
model.set_topic_labels(topic_labels)

model.get_topic_info()

document_ids = list(range(1, len(data) + 1))  # 1, 2, 3, ..., len(data)

# Create a mapping of topics to document IDs
topic_to_document_ids = {}
for topic, doc_id in zip(topics, document_ids):
    if topic not in topic_to_document_ids:
        topic_to_document_ids[topic] = []
    topic_to_document_ids[topic].append(doc_id)

# Convert the mapping to a DataFrame
topic_document_table = pd.DataFrame([
    {"Topic": topic, "Document_IDs": ", ".join(map(str, doc_ids))}
    for topic, doc_ids in topic_to_document_ids.items()
])

# Display the table
print(topic_document_table)

topic_document_table.to_csv("topic_to_document_ids.csv", index=False)

from bertopic.representation import KeyBERTInspired

# Fine-tune your topic representations
representation_model = KeyBERTInspired()
topic_model = BERTopic(representation_model=representation_model)

import openai
from bertopic.representation import OpenAI

# Fine-tune topic representations with GPT
client = openai.OpenAI(api_key="sk-...")
representation_model = OpenAI(client, model="gpt-3.5-turbo", chat=True)
topic_model = BERTopic(representation_model=representation_model)

model.get_document_info(data)

# Add topics to the DataFrame
df['Topic'] = topics

# Group by Topic and Year, then count the number of papers
grouped = df.groupby(['Topic', 'Publication Year']).size().reset_index(name='Paper_Count')

# Remove the topic labeled as -1 (outliers)
#grouped = grouped[grouped['Topic'] != -1]

# Pivot the table to get topics as rows and years as columns
pivot_table = grouped.pivot(index='Topic', columns='Publication Year', values='Paper_Count').fillna(0)

# Display the pivot table
print(pivot_table)

# Save the pivot table to a CSV file
pivot_table.to_csv('topic_year_pivot_table.csv')

# Add topics to the DataFrame
df['Topic'] = topics

# # Define custom bins for 5-year intervals with the last one as 4 years
year_bins = list(range(1975, 2025, 5))  # 5-year intervals
year_bins.append(2025)  # Ensure the last bin ends at 2024
year_labels = [f"{year}-{year + 4}" for year in year_bins[:-1]]  # Generate labels dynamically

# Ensure bins and labels align
assert len(year_bins) - 1 == len(year_labels), "Number of bins and labels must match"

# Debug: Print bins and labels
print("Year Bins:", year_bins)
print("Year Labels:", year_labels)

# Create Year_Interval column using pd.cut
df['Year_Interval'] = pd.cut(df['Publication Year'], bins=year_bins, labels=year_labels, right=False)

# Debug: Verify Year_Interval column
print(df[['Publication Year', 'Year_Interval']].value_counts())

# Group by Topic and Year_Interval, then count the number of papers
grouped = df.groupby(['Topic', 'Year_Interval']).size().reset_index(name='Paper_Count')

# Remove the topic labeled as -1 (outliers)
#grouped = grouped[grouped['Topic'] != -1]

# Pivot the table to get topics as rows and year intervals as columns
pivot_table = grouped.pivot(index='Topic', columns='Year_Interval', values='Paper_Count').fillna(0)

# Display the pivot table
print(pivot_table)

# Save the pivot table to a CSV file
pivot_table.to_csv('topic_year_interval_pivot_table.csv')

model.get_topic(0)

from nltk.corpus import stopwords
import nltk

# Download NLTK stopwords if not already present
nltk.download('stopwords')

# Define stop words
stop_words = set(stopwords.words('english'))

# Function to clean a single topic's keywords
def clean_topic_keywords(topic_model, topic_id, stop_words):
    # Retrieve the topic keywords
    keywords = topic_model.get_topic(topic_id)
    if not keywords:  # Skip empty topics
        return None
    # Remove stop words and create a meaningful topic phrase
    cleaned_keywords = [
        word for word, _ in keywords if word.lower() not in stop_words
    ]
    return " ".join(cleaned_keywords[:4])  # Use top 4 cleaned keywords for the label

# Clean all topics
def clean_all_topics(topic_model, stop_words):
    cleaned_topics = {}
    for topic_id in topic_model.get_topics().keys():
        #if topic_id != -1:  # Skip outliers
            cleaned_topics[topic_id] = clean_topic_keywords(topic_model, topic_id, stop_words)
    return cleaned_topics

# Retrieve cleaned topics
cleaned_topics = clean_all_topics(model, stop_words)

# Display cleaned topics
print("Cleaned Topics:")
for topic_id, keywords in cleaned_topics.items():
    print(f"Topic {topic_id}: {keywords}")

# Save cleaned topics to a CSV file
pd.DataFrame.from_dict(cleaned_topics, orient="index", columns=["Cleaned Keywords"]).to_csv("cleaned_topics.csv")

# Filter for the top 12 topics by total frequency
total_frequencies = pivot_table.sum(axis=1).sort_values(ascending=False)
top_topics = total_frequencies.head(12).index

# Filter the pivot table for the top topics
filtered_pivot_table = pivot_table.loc[top_topics]

# Plot the data
plt.figure(figsize=(12, 8))
filtered_pivot_table.T.plot(kind='bar', stacked=True, figsize=(12, 8))

# Add labels and title
plt.title("Topic Frequencies Across 5-Year Intervals")
plt.xlabel("5-Year Intervals")
plt.ylabel("Frequency of Papers")
plt.legend(title="Topics", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()

# Show the plot
plt.show()

# Save the filtered pivot table
filtered_pivot_table.to_csv("top_topics_5_year_intervals_with_last_4_year.csv")

import matplotlib.pyplot as plt
import pandas as pd

# Filter top 12 topics by frequency
topic_freq = model.get_topic_freq()
top_topics = topic_freq.sort_values(by="Count", ascending=False).head(12)["Topic"].tolist()

# Exclude the topic labeled as -1 (outliers) from top_topics
#top_topics = [topic for topic in top_topics if topic != -1]

# Define custom bins for 5-year intervals with the last one as 4 years
year_bins = list(range(1975, 2025, 5))  # 5-year intervals
year_bins.append(2025)  # Ensure the last bin ends at 2024
year_labels = [f"{year}-{year + 4}" for year in year_bins[:-2]] + ["2020-2024"]

# Ensure bins and labels align
assert len(year_bins) - 1 == len(year_labels), "Number of bins and labels must match"

# Create Year_Interval column using pd.cut
df['Year_Interval'] = pd.cut(df['Publication Year'], bins=year_bins, labels=year_labels, right=False)

# Regenerate the grouped data
grouped = df.groupby(['Topic', 'Year_Interval']).size().reset_index(name='Paper_Count')

# Remove the topic labeled as -1 (outliers)
#grouped = grouped[grouped['Topic'] != -1]

# Pivot the table to get topics as rows and year intervals as columns
pivot_table = grouped.pivot(index='Topic', columns='Year_Interval', values='Paper_Count').fillna(0)

# Filter the pivot table for top topics only
filtered_pivot_table = pivot_table.loc[top_topics]

# Map cleaned topics to the pivot table index for better labels
filtered_pivot_table.index = filtered_pivot_table.index.map(cleaned_topics)

# Display the filtered pivot table
print("Filtered Pivot Table with Cleaned Topics (Excluding Data Beyond 2024):")
print(filtered_pivot_table)

# Save the filtered pivot table
filtered_pivot_table.to_csv("top_topics_pivot_table_excluding_2025.csv")

# Visualize the data
plt.figure(figsize=(12, 8))
filtered_pivot_table.T.plot(kind='bar', stacked=True, figsize=(14, 8), width=0.8)

# Add labels and title
plt.title("Frequency of Top Topics Across 5-Year Intervals", fontsize=16)
plt.xlabel("Year Intervals", fontsize=14)
plt.ylabel("Paper Count", fontsize=14)
plt.legend(title="Topics", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()

# Save the bar chart
plt.savefig("top_topics_bar_chart_excluding_2025.png")

# Show the bar chart
plt.show()

model.get_topic(6)

model.get_topic(20)

model.get_topic(13)

model.visualize_topics()

model.visualize_barchart(width=280, height=330, top_n_topics=8, n_words=10)

model.visualize_heatmap(n_clusters=20)

model.visualize_documents(data)

language="english"

outliers_count = df[df["Topic"] == -1].shape[0]
total_documents = df.shape[0]
outliers_percentage = (outliers_count / total_documents) * 100
print(f"Outliers: {outliers_count} documents ({outliers_percentage:.2f}%)")

outlier_docs = df[df["Topic"] == -1]["combined_text"].head(10)
print(outlier_docs)

model.get_topic(-1)

outliers_percentage = (outliers_count / total_documents) * 100
print(f"Outliers: {outliers_percentage:.2f}%")

# Analyze the number and percentage of outliers
outliers_count = df[df["Topic"] == -1].shape[0]
total_documents = df.shape[0]
outliers_percentage = (outliers_count / total_documents) * 100

print(f"Outliers: {outliers_count} documents ({outliers_percentage:.2f}%)")

# Examine keywords of the outlier topic
outlier_keywords = model.get_topic(-1)
print("Outlier Keywords:", outlier_keywords)

# Sample outlier documents
outlier_docs = df[df["Topic"] == -1]["combined_text"].sample(10)
print("Sample Outlier Documents:", outlier_docs)

from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# â¶ Better vectorizer: remove stopwords, allow bigrams, ignore ultra-rare terms
vectorizer_model = CountVectorizer(
    stop_words="english",
    ngram_range=(1, 2),
    min_df=5
)

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# If you already have embeddings & topics, you can skip refit.
# But best results come from refitting with the improved vectorizer.
model = BERTopic(
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    language="english",
    calculate_probabilities=True
)
topics, probs = model.fit_transform(data)  # data = your list of docs

# Expand ontology prototypes to be more specific (edit text freely)
ONTOLOGY_TEXT = {
    "Defect_Prediction": "software defect prediction and bug prediction using metrics and change-proneness and JIT and static analysis",
    "Fault_Localization_Debugging": "fault localization, spectrum based fault localization, debugging, suspiciousness ranking, program spectra, Ochiai, Tarantula",
    "Code_Clone_Analysis": "code clone detection, clone analysis, near-duplicate code, clone evolution, clone refactoring",
    "Reliability_Modeling": "software reliability growth models, SRGM, failure rate, reliability prediction, NHPP models",
    "Anomaly_Log_Mining": "anomaly detection in system logs, log mining, log anomaly detection, sequence anomalies, log parsing",
    "Class_Imbalance_Methods": "class imbalance, SMOTE, oversampling, undersampling, imbalanced learning, minority class",
    "Topic_Modeling_Methods": "topic modeling with LDA, BERTopic, embeddings, clustering, topic coherence"
    # Add Requirements_Engineering, Testing_and_Validation, Maintenance if needed
}

cat_names = list(ONTOLOGY_TEXT.keys())
cat_vecs  = embedding_model.encode([ONTOLOGY_TEXT[c] for c in cat_names], show_progress_bar=False)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd

def topic_text(tid, k=15):
    items = model.get_topic(tid) or []
    words = [w for (w, _) in items[:k]]
    return " ".join(words)

def label_topics_by_cosine(threshold=0.32, k=15):  # slightly higher threshold
    topic_ids = [int(t) for t in model.get_topic_freq()["Topic"].tolist() if int(t) != -1]
    rows = []
    for tid in topic_ids:
        txt = topic_text(tid, k=k)
        if not txt.strip():
            rows.append((tid, "Unknown", 0.0, txt)); continue
        v = embedding_model.encode([txt], show_progress_bar=False)
        sims = cosine_similarity(v, cat_vecs)[0]
        best_i = int(np.argmax(sims))
        best_cat, best_sim = cat_names[best_i], float(sims[best_i])
        label = best_cat if best_sim >= threshold else "Unknown"
        rows.append((tid, label, round(best_sim, 3), txt))
    return pd.DataFrame(rows, columns=["Topic", "Symbolic_Label", "Label_Score", "Top_Words"])

symbolic_label_table = label_topics_by_cosine(threshold=0.32, k=15)
display(symbolic_label_table)
symbolic_label_table.to_csv("neurosymbolic_labels_cosine_v2.csv", index=False)

# Update map
topic_symbolic_label = dict(zip(symbolic_label_table.Topic, symbolic_label_table.Symbolic_Label))

# --- RUN THIS RIGHT AFTER your BERT-only labeling block ---
# You already ran:
# topic_labels = model.generate_topic_labels(...)
# model.set_topic_labels(topic_labels)

bert_info = model.get_topic_info()
# Keep non-outlier topics only
bert_info = bert_info[bert_info["Topic"] != -1].copy()

# BERTopic stores labels in the 'Name' column after set_topic_labels()
bert_labels_df = bert_info[["Topic", "Name"]].rename(columns={"Name": "Label"})

# (Optional) normalize label strings a bit
bert_labels_df["Label"] = bert_labels_df["Label"].str.strip()

# Save for later use
bert_labels_df.to_csv("bert_topic_labels.csv", index=False)
print("Saved â†’ bert_topic_labels.csv")

# This file is created by your NeSy labeling step:
# symbolic_label_table = label_topics_by_cosine(threshold=0.32, k=15)
# symbolic_label_table.to_csv("neurosymbolic_labels_cosine_v2.csv", index=False)

print("Expecting neurosymbolic_labels_cosine_v2.csv from the NeSy step.")

import pandas as pd

# Load BERT-only and NeSy label tables
bert_labels_df = pd.read_csv("bert_topic_labels.csv")  # columns: Topic, Label
nesy_labels_df = pd.read_csv("neurosymbolic_labels_cosine_v2.csv")  # columns: Topic, Symbolic_Label, Label_Score, Top_Words

# --- Count topics per category for each side ---
bert_counts = bert_labels_df["Label"].value_counts().rename("BERT-only")
nesy_counts = nesy_labels_df["Symbolic_Label"].value_counts().rename("NeSy")

# Union of categories from both sides
all_cats = sorted(set(bert_counts.index).union(set(nesy_counts.index)))

# Combined comparison table
comparison = pd.concat(
    [bert_counts.reindex(all_cats), nesy_counts.reindex(all_cats)],
    axis=1
).fillna(0).astype(int)

# Total row
comparison.loc["Total_Topics"] = comparison.sum()

# Optional: brief observations (edit freely)
comparison["Observations"] = ""
for cat in comparison.index:
    if cat == "Total_Topics":
        continue
    if cat.lower().startswith("defect"):
        comparison.at[cat, "Observations"] = "Core SDP theme; often inflated in BERT-only"
    elif "Localization" in cat or "Debugging" in cat:
        comparison.at[cat, "Observations"] = "Separated by NeSy (hidden under DP before)"
    elif "Clone" in cat:
        comparison.at[cat, "Observations"] = "NeSy surfaced clone analysis"
    elif "Reliability" in cat:
        comparison.at[cat, "Observations"] = "Reliability modeling emerged via NeSy"
    elif "Imbalance" in cat:
        comparison.at[cat, "Observations"] = "Class imbalance (SMOTE) captured by NeSy"
    elif "Anomaly" in cat or "Log" in cat:
        comparison.at[cat, "Observations"] = "Log/anomaly mining separated by NeSy"
    elif cat == "Unknown":
        comparison.at[cat, "Observations"] = "Ambiguous topics (leave for human review)"

# Save & show
comparison.to_csv("bert_vs_nesy_topic_comparison.csv")
print("Saved â†’ bert_vs_nesy_topic_comparison.csv")
print(comparison)

import pandas as pd
import matplotlib.pyplot as plt

# Load comparison table you already built
comparison = pd.read_csv("bert_vs_nesy_topic_comparison.csv", index_col=0)

# Drop noisy raw BERT labels (keep only high-level categories that appear in NeSy)
categories = ["Defect_Prediction","Fault_Localization_Debugging","Code_Clone_Analysis",
              "Reliability_Modeling","Class_Imbalance_Methods","Anomaly_Log_Mining","Unknown"]

subset = comparison.loc[categories, ["BERT-only","NeSy"]]

# Plot
subset.plot(kind="bar", figsize=(10,6))
plt.title("Topic Distribution: BERT-only vs NeSy", fontsize=14)
plt.ylabel("Number of Topics")
plt.xlabel("Category")
plt.xticks(rotation=30, ha="right")
plt.legend(title="Model")
plt.tight_layout()
plt.show()

subset.plot(kind="barh", stacked=True, figsize=(10,6))
plt.title("Reorganization of Topics by NeSy vs BERT", fontsize=14)
plt.xlabel("Number of Topics")
plt.ylabel("Category")
plt.tight_layout()
plt.show()

import seaborn as sns

# Example pivot: for now use just the counts table (you can refine later with doc-level mapping)
pivot = subset.copy()

plt.figure(figsize=(8,6))
sns.heatmap(pivot, annot=True, cmap="Blues", fmt="d")
plt.title("BERT-only vs NeSy Topic Distribution Heatmap", fontsize=14)
plt.ylabel("Category")
plt.xlabel("Model")
plt.show()

import plotly.graph_objects as go
import pandas as pd

# Load mappings: each BERT topic id -> NeSy category
bert_labels_df = pd.read_csv("bert_topic_labels.csv")              # Topic, Label
nesy_labels_df = pd.read_csv("neurosymbolic_labels_cosine_v2.csv") # Topic, Symbolic_Label

# Merge on Topic ID (align topics)
merged = pd.merge(bert_labels_df, nesy_labels_df[["Topic","Symbolic_Label"]],
                  left_on="Topic", right_on="Topic")

# Build node list (unique labels)
bert_nodes = merged["Label"].unique().tolist()
nesy_nodes = merged["Symbolic_Label"].unique().tolist()
nodes = bert_nodes + nesy_nodes

# Map names to indices
node_index = {name: i for i, name in enumerate(nodes)}

# Build links
links = []
for _, row in merged.iterrows():
    source = node_index[row["Label"]]
    target = node_index[row["Symbolic_Label"]]
    links.append((source, target))

# Count flows
from collections import Counter
counts = Counter(links)

# Prepare for Sankey
sources, targets, values = zip(*[(s, t, c) for (s,t), c in counts.items()])

fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15, thickness=20, line=dict(color="black", width=0.5),
        label=nodes
    ),
    link=dict(source=sources, target=targets, value=values)
)])

fig.update_layout(title_text="BERT vs NeSy Topic Flow", font_size=12)
fig.show()

# === BERT â†’ NeSy Sankey: actual outputs ===
import pandas as pd
from collections import Counter
import plotly.graph_objects as go

# 1) Load your two files created earlier
#   - bert_topic_labels.csv: columns ["Topic","Label"]  (after model.set_topic_labels)
#   - neurosymbolic_labels_cosine_v2.csv: columns ["Topic","Symbolic_Label", "Label_Score", "Top_Words"]
bert_df = pd.read_csv("bert_topic_labels.csv")
nesy_df = pd.read_csv("neurosymbolic_labels_cosine_v2.csv")

# 2) Clean & align
# drop outlier cluster -1 if present
bert_df = bert_df[bert_df["Topic"] != -1].copy()
nesy_df = nesy_df[nesy_df["Topic"] != -1].copy()

# shorten long BERT labels for readability (first 3 tokens)
def shorten(lbl, max_tokens=3):
    toks = str(lbl).strip().split()
    short = "_".join(toks[:max_tokens]).lower()
    return short if short else "topic"

bert_df["BERT_Label_Short"] = bert_df["Label"].apply(shorten)

# 3) Merge BERT topic id â†’ NeSy category
m = pd.merge(bert_df[["Topic","BERT_Label_Short"]],
             nesy_df[["Topic","Symbolic_Label"]],
             on="Topic", how="inner")

# 4) Build link counts (some BERT labels may repeat after shortening; thatâ€™s ok)
pairs = list(zip(m["BERT_Label_Short"], m["Symbolic_Label"]))
counts = Counter(pairs)

# 5) Create node list (left BERT labels first, then NeSy categories)
left_nodes  = sorted(m["BERT_Label_Short"].unique().tolist())
right_nodes = sorted(m["Symbolic_Label"].unique().tolist())
nodes = left_nodes + right_nodes

node_index = {name: i for i, name in enumerate(nodes)}

# 6) Build source/target/value arrays
sources, targets, values = [], [], []
for (src_lbl, dst_lbl), val in counts.items():
    sources.append(node_index[src_lbl])
    targets.append(node_index[dst_lbl])
    values.append(val)

# 7) Pretty labels: keep left prefixed with "BERT: " and right with "NeSy: "
node_labels = [f"BERT: {s}" for s in left_nodes] + [f"NeSy: {s}" for s in right_nodes]

# Optional: simple colors â€” left side one color, right side another
left_color  = "rgba(100,149,237,0.7)"  # cornflowerblue
right_color = "rgba(60,179,113,0.7)"   # mediumseagreen
node_colors = [left_color]*len(left_nodes) + [right_color]*len(right_nodes)

fig = go.Figure(data=[go.Sankey(
    arrangement="snap",
    node=dict(
        pad=12,
        thickness=16,
        line=dict(color="rgba(0,0,0,0.3)", width=0.5),
        label=node_labels,
        color=node_colors,
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values
    )
)])

fig.update_layout(
    title_text="Flow of Topics: BERT-only labels â†’ NeSy categories",
    font_size=12,
    height=700
)

# Show inline (Jupyter) and save an interactive HTML
fig.show()
fig.write_html("bert_to_nesy_sankey.html")
print("Saved â†’ bert_to_nesy_sankey.html")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# -----------------------------
# 1) Load base dataset
# -----------------------------
df = pd.read_excel(
    "Software defect Prediction.xlsx",
    usecols=["Publication Year", "Title", "Abstract Note"],
    sheet_name="Sheet1"
).copy()
df["combined_text"] = df["Title"].fillna("") + " " + df["Abstract Note"].fillna("")
df["DocID"] = np.arange(len(df))

# Keep only rows with a valid year
df = df[df["Publication Year"].notna()].copy()
df["Year"] = df["Publication Year"].astype(int)

# -----------------------------
# 2) Attach Topic IDs to df
#    Try, in order:
#      a) documents_with_neurosymbolic_labels.csv  (Topic or Topic_NS)
#      b) in-memory `topics` variable
# -----------------------------
topic_attached = False

docs_csv = Path("documents_with_neurosymbolic_labels.csv")
if docs_csv.exists():
    docs_df = pd.read_csv(docs_csv)
    # Best-effort match on length and columns
    if len(docs_df) == len(df):
        if "Topic" in docs_df.columns:
            df["Topic"] = docs_df["Topic"].values
            topic_attached = True
        elif "Topic_NS" in docs_df.columns:
            df["Topic"] = docs_df["Topic_NS"].values
            topic_attached = True

if not topic_attached:
    # Fallback to in-memory `topics` if present
    try:
        df["Topic"] = topics  # uses the variable defined when you fit BERTopic
        topic_attached = True
    except NameError:
        topic_attached = False

if not topic_attached:
    raise RuntimeError(
        "No topic assignments found. Please ensure either "
        "'documents_with_neurosymbolic_labels.csv' exists with Topic/Topic_NS, "
        "or the variable `topics` is in memory."
    )

# -----------------------------
# 3) Map NeSy categories (from your cosine-labeling output)
# -----------------------------
nesy_file = Path("neurosymbolic_labels_cosine_v2.csv")
if not nesy_file.exists():
    raise RuntimeError(
        "Missing 'neurosymbolic_labels_cosine_v2.csv'. Run the NeSy cosine-labeling "
        "cell that saves this file before running trends."
    )

nesy_labels_df = pd.read_csv(nesy_file)   # columns: Topic, Symbolic_Label, Label_Score, Top_Words
# Ensure we only use non-outlier topics if your NeSy table excluded -1
nesy_map = dict(zip(nesy_labels_df["Topic"].astype(int), nesy_labels_df["Symbolic_Label"]))
df["NeSy_Label"] = df["Topic"].astype(int).map(nesy_map).fillna("Unknown")

# -----------------------------
# 4) (Optional) Map BERT labels (if you saved them)
# -----------------------------
bert_labels_path = Path("bert_topic_labels.csv")
if bert_labels_path.exists():
    bert_labels_df = pd.read_csv(bert_labels_path)  # columns: Topic, Label
    bert_map = dict(zip(bert_labels_df["Topic"].astype(int), bert_labels_df["Label"]))
    df["BERT_Label"] = df["Topic"].astype(int).map(bert_map).fillna("Unknown")
else:
    df["BERT_Label"] = np.nan  # not available

# -----------------------------
# 5) Build trend (counts per year)
# -----------------------------
# NeSy trends
trend_nesy = (
    df.groupby(["Year", "NeSy_Label"])
      .size()
      .reset_index(name="Count")
      .pivot(index="Year", columns="NeSy_Label", values="Count")
      .fillna(0)
      .sort_index()
)

# BERT trends (only if labels available)
if df["BERT_Label"].notna().any():
    trend_bert = (
        df.groupby(["Year", "BERT_Label"])
          .size()
          .reset_index(name="Count")
          .pivot(index="Year", columns="BERT_Label", values="Count")
          .fillna(0)
          .sort_index()
    )
else:
    trend_bert = None

# -----------------------------
# 6) Plot NeSy trends (absolute + relative)
# -----------------------------
plt.figure(figsize=(12,6))
trend_nesy.plot(kind="line", marker="o", ax=plt.gca())
plt.title("Trend of NeSy Topic Categories Over Time")
plt.ylabel("Number of Papers")
plt.xlabel("Publication Year")
plt.legend(title="NeSy Categories", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

plt.figure(figsize=(12,6))
trend_nesy_pct = trend_nesy.div(trend_nesy.sum(axis=1), axis=0).fillna(0)
trend_nesy_pct.plot(kind="area", stacked=True, alpha=0.85, ax=plt.gca())
plt.title("Relative Share of NeSy Categories Over Time")
plt.ylabel("Proportion of Papers")
plt.xlabel("Publication Year")
plt.legend(title="NeSy Categories", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# -----------------------------
# 7) (Optional) Plot BERT trends for comparison
# -----------------------------
if trend_bert is not None:
    # For readability, plot only the top 8 BERT labels by total count
    top_bert = trend_bert.sum(axis=0).sort_values(ascending=False).head(8).index
    plt.figure(figsize=(12,6))
    trend_bert[top_bert].plot(kind="line", marker="o", ax=plt.gca())
    plt.title("Trend of BERT-only Topic Labels (Top 8) Over Time")
    plt.ylabel("Number of Papers")
    plt.xlabel("Publication Year")
    plt.legend(title="BERT Labels", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()

# -----------------------------
# 8) Save trend tables
# -----------------------------
trend_nesy.to_csv("trend_nesy_counts_by_year.csv")
if trend_bert is not None:
    trend_bert.to_csv("trend_bert_counts_by_year.csv")

print("Saved â†’ trend_nesy_counts_by_year.csv",
      "and trend_bert_counts_by_year.csv" if trend_bert is not None else "")

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Assumes you already have:
# df  -> your documents DataFrame with a "Publication Year" column
# topics, probs -> from BERTopic fit_transform(data)

df["Topic"] = topics
df["Year"] = df["Publication Year"].astype(int)

# --- 1. Compute average topic weight per year ---
# probs is a 2D array (num_docs x num_topics)
probs_df = pd.DataFrame(probs)
probs_df["Year"] = df["Year"].values
probs_year = probs_df.groupby("Year").mean()

# --- 2. Plot per-topic trends ---
n_topics = probs_year.shape[1]
ncols = 3
nrows = int(np.ceil(n_topics / ncols))

fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows*3), sharex=True)
axes = axes.flatten()

for i in range(n_topics):
    axes[i].plot(probs_year.index, probs_year[i], marker="o", linewidth=1)
    axes[i].set_title(f"Topic {i}")
    axes[i].set_ylabel("Average Topic Weight")
    axes[i].set_xlabel("Publication Year")

plt.suptitle("Topic Trends by Average Topic Weight", fontsize=16)
plt.tight_layout(rect=[0,0,1,0.97])
plt.savefig("topic_trends_by_average_topic_weight.png", dpi=300, bbox_inches='tight')
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Assumes you already have:
# df  -> your documents DataFrame with a "Publication Year" column
# topics, probs -> from BERTopic fit_transform(data)

df["Topic"] = topics
df["Year"] = df["Publication Year"].astype(int)


# --- 3. Rolling mean (5-year window) version ---
rolling = probs_year.rolling(window=5, min_periods=1).mean()

fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows*3), sharex=True)
axes = axes.flatten()

for i in range(n_topics):
    axes[i].plot(rolling.index, rolling[i], marker="o", color="darkred")
    axes[i].set_title(f"Topic {i} (5-Year Rolling Mean)")
    axes[i].set_ylabel("Average Rolled Value")
    axes[i].set_xlabel("Year")

plt.suptitle("Topic Trends (5-Year Rolling Mean)", fontsize=16)
plt.tight_layout(rect=[0,0,1,0.97])
plt.savefig("topic_trends_5year_rolling.png", dpi=300, bbox_inches='tight')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Assumes you already have df, topics, probs from BERTopic
df["Topic"] = topics
df["Year"] = df["Publication Year"].astype(int)

# --- 1. Compute average topic weight per year ---
probs_df = pd.DataFrame(probs)
probs_df["Year"] = df["Year"].values
probs_year = probs_df.groupby("Year").mean()

# --- 2. Rolling mean (5-year window) ---
rolling = probs_year.rolling(window=5, min_periods=1).mean()

# --- 3. Plot both in one combined figure ---
n_topics = probs_year.shape[1]
ncols = 3
nrows = int(np.ceil(n_topics / ncols))

fig, axes = plt.subplots(2*nrows, ncols, figsize=(15, nrows*6), sharex=True)
axes = axes.flatten()

# Top row set: raw averages
for i in range(n_topics):
    axes[i].plot(probs_year.index, probs_year[i], marker="o", linewidth=1)
    axes[i].set_title(f"Topic {i} - Avg Weight")
    axes[i].set_ylabel("Average Topic Weight")
    axes[i].set_xlabel("Publication Year")

# Bottom row set: rolling mean
offset = n_topics
for i in range(n_topics):
    axes[offset+i].plot(rolling.index, rolling[i], marker="o", color="darkred")
    axes[offset+i].set_title(f"Topic {i} - 5Y Rolling Mean")
    axes[offset+i].set_ylabel("Average Rolled Value")
    axes[offset+i].set_xlabel("Year")

plt.suptitle("Topic Trends (Average vs Rolling Mean)", fontsize=18)
plt.tight_layout(rect=[0,0,1,0.96])

# --- 4. Save as one PNG file ---
plt.savefig("topic_trends_combined.png", dpi=300)
plt.close()

print("âœ… Combined topic trends saved â†’ topic_trends_combined.png")

# ==============================
# NeuroSymbolic Knowledge Graph (with PNG exports)
# Documents â†’ Topics â†’ Categories (+ top words)
# ==============================

import sys, subprocess, re, random
def pip_install(pkgs):
    subprocess.run([sys.executable, "-m", "pip", "install", "-q"] + pkgs, check=False)

# Try to import; install if missing
for pkg in ["rdflib", "networkx", "pyvis", "pandas", "matplotlib"]:
    try:
        __import__(pkg)
    except ImportError:
        pip_install([pkg])

import pandas as pd
import networkx as nx
from rdflib import Graph, Namespace, Literal, RDF, RDFS, XSD
from pyvis.network import Network
from pathlib import Path
import matplotlib.pyplot as plt

# --------- 0) PNG controls ----------
# How many document nodes to include in the full PNG (too many = unreadable)
MAX_DOCS_IMAGE = 300     # tune this number
RANDOM_SEED = 42

# --------- 1) Load your sources ----------
# Base documents (must have Publication Year, Title, Abstract Note)
df = pd.read_excel(
    "Software defect Prediction.xlsx",
    usecols=["Publication Year", "Title", "Abstract Note"],
    sheet_name="Sheet1"
).copy()

df["combined_text"] = df["Title"].fillna("") + " " + df["Abstract Note"].fillna("")
df["Year"] = df["Publication Year"].fillna(0).astype(int)
df["DocID"] = range(len(df))

# Topic assignments (from your BERTopic run)
topic_attached = False
doc_map_file = Path("documents_with_neurosymbolic_labels.csv")
if doc_map_file.exists():
    m = pd.read_csv(doc_map_file)
    if "Topic" in m.columns and len(m) == len(df):
        df["Topic"] = m["Topic"].astype(int).values
        topic_attached = True
    elif "Topic_NS" in m.columns and len(m) == len(df):
        df["Topic"] = m["Topic_NS"].astype(int).values
        topic_attached = True

if not topic_attached:
    try:
        df["Topic"] = topics           # from model.fit_transform(data)
        topic_attached = True
    except NameError:
        raise RuntimeError("No topic assignments found. Provide 'documents_with_neurosymbolic_labels.csv' or ensure 'topics' exists.")

# NeSy topic labels (from your cosine prototype step)
nesy_file = Path("neurosymbolic_labels_cosine_v2.csv")
if not nesy_file.exists():
    raise RuntimeError("Missing 'neurosymbolic_labels_cosine_v2.csv'. Run the NeSy labeling cell that saves this file.")

nesy = pd.read_csv(nesy_file)  # columns: Topic, Symbolic_Label, Label_Score, Top_Words
nesy["Topic"] = nesy["Topic"].astype(int)

# Make a clean Top_Words list
import re as _re
def parse_top_words(s, k=10):
    toks = _re.split(r"[,\s]+", str(s))
    toks = [w for w in toks if _re.search(r"[A-Za-z][A-Za-z0-9_\-\+]*", w)]
    bad = {"the","of","and","in","to","we","for","on","a","an"}
    toks = [w for w in toks if w.lower() not in bad]
    return toks[:k]

nesy["TopWordsList"] = nesy["Top_Words"].apply(parse_top_words)

# Maps
t2cat   = dict(zip(nesy["Topic"], nesy["Symbolic_Label"]))
t2score = dict(zip(nesy["Topic"], nesy["Label_Score"]))
t2words = dict(zip(nesy["Topic"], nesy["TopWordsList"]))

# Attach category to documents
df["Category"] = df["Topic"].map(t2cat).fillna("Unknown")

# --------- 2) Build an RDF Knowledge Graph ----------
EX = Namespace("http://example.org/sdp#")
KG = Graph()
KG.bind("ex", EX); KG.bind("rdfs", RDFS)

# Classes
KG.add((EX.Document, RDF.type, RDFS.Class))
KG.add((EX.Topic,    RDF.type, RDFS.Class))
KG.add((EX.Category, RDF.type, RDFS.Class))
KG.add((EX.Keyword,  RDF.type, RDFS.Class))

# Properties
props = {
    "hasTopic": EX.hasTopic,
    "hasCategory": EX.hasCategory,
    "hasTopWord": EX.hasTopWord,
    "hasLabelScore": EX.hasLabelScore,
    "publishedInYear": EX.publishedInYear,
    "title": EX.title
}
for p in props.values():
    KG.add((p, RDF.type, RDF.Property))

# Category nodes
categories = sorted(set(t2cat.values()) | {"Unknown"})
def cat_uri(c): return EX[f"Category_{_re.sub('[^A-Za-z0-9_]+','_', c)}"]
for c in categories:
    KG.add((cat_uri(c), RDF.type, EX.Category))
    KG.add((cat_uri(c), RDFS.label, Literal(c)))

# Topic nodes
for _, row in nesy.iterrows():
    tid = int(row["Topic"])
    if tid == -1:  # skip outlier
        continue
    label = row["Symbolic_Label"]; score = float(row["Label_Score"]); words = row["TopWordsList"]
    t_uri = EX[f"Topic_{tid}"]
    KG.add((t_uri, RDF.type, EX.Topic))
    KG.add((t_uri, RDFS.label, Literal(f"Topic {tid}")))
    KG.add((t_uri, props["hasCategory"], cat_uri(label)))
    KG.add((t_uri, props["hasLabelScore"], Literal(score)))

    for w in words:
        kw_uri = EX[f"Keyword_{_re.sub(r'[^A-Za-z0-9_]+','_', w.lower())}"]
        KG.add((kw_uri, RDF.type, EX.Keyword))
        KG.add((kw_uri, RDFS.label, Literal(w)))
        KG.add((t_uri, props["hasTopWord"], kw_uri))

# Document nodes
for _, r in df.iterrows():
    doc_uri = EX[f"Doc_{int(r['DocID'])}"]
    KG.add((doc_uri, RDF.type, EX.Document))
    KG.add((doc_uri, props["title"], Literal(str(r["Title"]))))
    KG.add((doc_uri, props["publishedInYear"], Literal(int(r["Year"]))))
    tid = int(r["Topic"])
    if tid != -1:
        KG.add((doc_uri, props["hasTopic"], EX[f"Topic_{tid}"]))
    KG.add((doc_uri, props["hasCategory"], cat_uri(r["Category"])))

KG.serialize(destination="nesy_kg.ttl", format="turtle")
print("âœ… RDF knowledge graph saved â†’ nesy_kg.ttl")

# --------- 3) Build a NetworkX graph & GraphML ----------
G = nx.DiGraph()

# Add categories
for c in categories:
    cid = f"Category::{c}"
    G.add_node(cid, type="Category", label=c)

# Add topics
for _, row in nesy.iterrows():
    tid = int(row["Topic"])
    if tid == -1:
        continue
    tnode = f"Topic::{tid}"
    G.add_node(tnode, type="Topic", label=f"Topic {tid}", score=float(row["Label_Score"]))
    G.add_edge(tnode, f"Category::{row['Symbolic_Label']}", relation="belongsTo")
    for w in row["TopWordsList"]:
        knode = f"Keyword::{w.lower()}"
        if knode not in G:
            G.add_node(knode, type="Keyword", label=w)
        G.add_edge(tnode, knode, relation="hasTopWord")

# Add documents (lightweight)
for _, r in df.iterrows():
    dnode = f"Doc::{int(r['DocID'])}"
    G.add_node(dnode, type="Document", label=str(r["Title"])[:80], year=int(r["Year"]))
    if int(r["Topic"]) != -1:
        G.add_edge(dnode, f"Topic::{int(r['Topic'])}", relation="hasTopic")
    G.add_edge(dnode, f"Category::{r['Category']}", relation="hasCategory")

nx.write_graphml(G, "nesy_kg.graphml")
print("âœ… GraphML saved â†’ nesy_kg.graphml")

# --------- 4) Interactive HTML (PyVis) ----------
net = Network(height="750px", width="100%", bgcolor="#ffffff", font_color="#111")
net.force_atlas_2based()

colors = {"Category":"#2ca02c", "Topic":"#1f77b4", "Keyword":"#9467bd", "Document":"#ff7f0e"}
for n, attrs in G.nodes(data=True):
    t = attrs.get("type", "Node")
    net.add_node(
        n,
        label=attrs.get("label", n),
        title=f"{t}",
        color=colors.get(t, "#7f7f7f"),
        physics=True
    )
for u, v, attrs in G.edges(data=True):
    net.add_edge(u, v, title=attrs.get("relation",""))

net.save_graph("nesy_kg.html")
print("âœ… Interactive HTML graph saved â†’ nesy_kg.html")

# --------- 5) STATIC PNGs with matplotlib ----------
def draw_and_save_png(Gsub, path, title="Graph", seed=RANDOM_SEED):
    plt.figure(figsize=(16, 12))
    # Layout: Kamada-Kawai often better for small/mid graphs; fallback to spring
    try:
        pos = nx.kamada_kawai_layout(Gsub, weight=None)
    except Exception:
        pos = nx.spring_layout(Gsub, seed=seed, k=None)

    # Node styling by type
    type2color = {"Category":"#2ca02c", "Topic":"#1f77b4", "Keyword":"#9467bd", "Document":"#ff7f0e"}
    type2size  = {"Category":1400, "Topic":900, "Keyword":500, "Document":300}

    node_colors = []
    node_sizes  = []
    labels = {}
    for n, attrs in Gsub.nodes(data=True):
        t = attrs.get("type", "Node")
        node_colors.append(type2color.get(t, "#7f7f7f"))
        node_sizes.append(type2size.get(t, 300))
        # show labels only for Category/Topic to keep image readable
        if t in {"Category","Topic"}:
            labels[n] = attrs.get("label", n)

    nx.draw_networkx_edges(Gsub, pos, alpha=0.25, width=0.8, arrows=True, arrowstyle="-")
    nx.draw_networkx_nodes(Gsub, pos, node_color=node_colors, node_size=node_sizes, linewidths=0.5, edgecolors="#333333")
    nx.draw_networkx_labels(Gsub, pos, labels=labels, font_size=9)

    plt.title(title, fontsize=16)
    plt.axis("off")
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()
    print(f"ðŸ–¼ï¸  PNG saved â†’ {path}")

# (a) Categoryâ€“Topicâ€“Keyword subgraph (no documents)
nodes_ctk = [n for n, a in G.nodes(data=True) if a.get("type") in {"Category","Topic","Keyword"}]
G_ctk = G.subgraph(nodes_ctk).copy()
draw_and_save_png(G_ctk, "nesy_kg_categories_topics_keywords.png",
                  title="NeSy KG: Categoriesâ€“Topicsâ€“Keywords")

# (b) Full graph with a sample of documents
random.seed(RANDOM_SEED)
doc_nodes = [n for n, a in G.nodes(data=True) if a.get("type") == "Document"]
if len(doc_nodes) > MAX_DOCS_IMAGE:
    doc_nodes_sample = random.sample(doc_nodes, MAX_DOCS_IMAGE)
else:
    doc_nodes_sample = doc_nodes

keep_nodes = set(nodes_ctk) | set(doc_nodes_sample)
# Also keep edges from sampled docs to their neighbors
for d in doc_nodes_sample:
    keep_nodes.update(G.successors(d))
    keep_nodes.update(G.predecessors(d))

G_full_sampled = G.subgraph(list(keep_nodes)).copy()
draw_and_save_png(G_full_sampled, "nesy_kg_full_sampled.png",
                  title=f"NeSy KG (sampled docs â‰¤ {MAX_DOCS_IMAGE})")

print(f"Nodes: {G.number_of_nodes()} | Edges: {G.number_of_edges()}")